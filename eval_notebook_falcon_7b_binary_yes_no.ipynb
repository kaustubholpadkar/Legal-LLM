{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import openai\n",
    "import os\n",
    "import sklearn.metrics\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m API key set.\n",
      "INFO:baseten:API key set.\n"
     ]
    }
   ],
   "source": [
    "import baseten\n",
    "baseten.login(\"yiVOous9.2mjkPJSPdya6FUFbGKmXaLyUF6ZxTQYs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_utils import TASKS, load_data, load_prompt, generate_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your LLM stack goes here\n",
    "\n",
    "# Falcon 7B\n",
    "\n",
    "model = baseten.deployed_model_version_id('q049013')\n",
    "\n",
    "def call_llm(prompt):\n",
    "    output = model.predict({\"prompt\": prompt})\n",
    "    output = output[\"data\"][\"generated_text\"]\n",
    "    result = output.rsplit(\" \", maxsplit=1)[-1]\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'generated_text': 'what is life?\\nLife is a complex and dynamic phenomenon that is characterized by the presence of living'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict({\"prompt\": \"what is life?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tasks: list, tasks_dir: str):\n",
    "    report = dict()\n",
    "    for task in tqdm(tasks):\n",
    "        train_df, test_df = load_data(task=task, tasks_dir=tasks_dir)\n",
    "        prompt_template = load_prompt(prompt_name=\"base_prompt.txt\", task=task, tasks_dir=tasks_dir)\n",
    "        prompts = generate_prompts(prompt_template=prompt_template, data_df=train_df)\n",
    "        report[task] = dict()\n",
    "        targets = list()\n",
    "        outputs = list()\n",
    "        for prompt, data in zip(prompts, train_df.iterrows()):\n",
    "            datapoint_id, data = data\n",
    "            output = call_llm(prompt)\n",
    "            output = output.strip()\n",
    "            targets.append(data['answer'])\n",
    "            outputs.append(output)\n",
    "            success = output == data['answer']\n",
    "            report[task][datapoint_id] = {\n",
    "                'prompt': prompt,\n",
    "                'generated_output': output,\n",
    "                'correct_output': data['answer'],\n",
    "                'success': output == data['answer']\n",
    "            }\n",
    "        report[task]['balanced_accuracy'] = sklearn.metrics.balanced_accuracy_score(targets, outputs)\n",
    "    \n",
    "    print('Balanced Accuracy:', sum([report[task]['balanced_accuracy'] if not math.isnan(report[task]['balanced_accuracy']) else 0 for task in tasks])/len(tasks))\n",
    "    \n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TASKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_dir = '../legalbench'\n",
    "\n",
    "# tasks=random.sample(TASKS, 10)\n",
    "\n",
    "# tasks= [task for task in TASKS if task.startswith(\"cuad\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"task_label_dict.json\", \"r\") as f:\n",
    "    task_label_dict = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_yes_no_tasks = [task for task in task_label_dict.keys() if task_label_dict[task][\"type\"] == \"binary_yes_no\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(binary_yes_no_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = binary_yes_no_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–Ž         | 4/109 [00:13<07:12,  4.12s/it]"
     ]
    }
   ],
   "source": [
    "report = evaluate(tasks=tasks, tasks_dir=tasks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Balanced Accuracy:', sum([report[task]['balanced_accuracy'] if not math.isnan(report[task]['balanced_accuracy']) else 0 for task in tasks])/len(tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
